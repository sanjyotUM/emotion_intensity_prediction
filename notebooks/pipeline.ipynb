{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import emoji\n",
    "import unidecode\n",
    "import spacy\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 500)\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../datasets/emotion_detection_semeval2017/combined/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>10894</td>\n",
       "      <td>i live and die for mchanzo honeymoon crashing and burning the second they move in together</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2318</td>\n",
       "      <td>30675</td>\n",
       "      <td>All the fans wanted Man Utd at home in the next round...\\nAre you cheering for Northampton or Man Utd right now?\\n#lufc</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>30165</td>\n",
       "      <td>A joyous first webiversary/web mitzvah to Smithsonian's @WeiPoints!! @brianwolly @jackie_mansky @bethpylieberman @bilbo @mazeltov</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3494</td>\n",
       "      <td>21028</td>\n",
       "      <td>@PanicAtTheDisco hey, y'all announced it like immediately after I asked. Nice. Thanks y'all</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1533</td>\n",
       "      <td>40676</td>\n",
       "      <td>Do not be discouraged by a slowing sales market. This will test your business model and pinpoint #strengths and #weaknesses.' @Ken_Dunn</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>10919</td>\n",
       "      <td>Why to have vanity sizes?Now sizes S,XS(evenXXS sometimes) are too big, WTF?! Dear corporate jerks, Lithuania didn't need this. #rant #angry</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2833</td>\n",
       "      <td>20367</td>\n",
       "      <td>@BaileyDemented @hsmitty3 ill kill u if u bully her ðŸ˜¤ðŸ˜¤ðŸ˜¤</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3512</td>\n",
       "      <td>21046</td>\n",
       "      <td>Not sure that men can handle a woman that's got her crap together.  #independent</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2186</td>\n",
       "      <td>30543</td>\n",
       "      <td>lol! no mention of pak PM or even his speech on any international news channel and pakis are rejoicing as if the world stands with them</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>40421</td>\n",
       "      <td>@DxfyingGrxvity - that were rather forlorn, scanning the witches house before resting back on Elphie. 'The Grimmerie is gone.'</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "3650  10894   \n",
       "2318  30675   \n",
       "1808  30165   \n",
       "3494  21028   \n",
       "1533  40676   \n",
       "3675  10919   \n",
       "2833  20367   \n",
       "3512  21046   \n",
       "2186  30543   \n",
       "1278  40421   \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "3650                                                    i live and die for mchanzo honeymoon crashing and burning the second they move in together   \n",
       "2318                       All the fans wanted Man Utd at home in the next round...\\nAre you cheering for Northampton or Man Utd right now?\\n#lufc   \n",
       "1808             A joyous first webiversary/web mitzvah to Smithsonian's @WeiPoints!! @brianwolly @jackie_mansky @bethpylieberman @bilbo @mazeltov   \n",
       "3494                                                  @PanicAtTheDisco hey, y'all announced it like immediately after I asked. Nice. Thanks y'all    \n",
       "1533       Do not be discouraged by a slowing sales market. This will test your business model and pinpoint #strengths and #weaknesses.' @Ken_Dunn   \n",
       "3675  Why to have vanity sizes?Now sizes S,XS(evenXXS sometimes) are too big, WTF?! Dear corporate jerks, Lithuania didn't need this. #rant #angry   \n",
       "2833                                                                                       @BaileyDemented @hsmitty3 ill kill u if u bully her ðŸ˜¤ðŸ˜¤ðŸ˜¤   \n",
       "3512                                                              Not sure that men can handle a woman that's got her crap together.  #independent   \n",
       "2186       lol! no mention of pak PM or even his speech on any international news channel and pakis are rejoicing as if the world stands with them   \n",
       "1278                @DxfyingGrxvity - that were rather forlorn, scanning the witches house before resting back on Elphie. 'The Grimmerie is gone.'   \n",
       "\n",
       "      emotion  intensity  \n",
       "3650    anger      0.479  \n",
       "2318      joy      0.292  \n",
       "1808      joy      0.667  \n",
       "3494     fear      0.250  \n",
       "1533  sadness      0.271  \n",
       "3675    anger      0.708  \n",
       "2833     fear      0.583  \n",
       "3512     fear      0.229  \n",
       "2186      joy      0.396  \n",
       "1278  sadness      0.458  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweet = \"The moment you bring her to meet your best friend and you're nervous af! ðŸ˜¬ðŸ˜† #nervous #thefriendtest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjyot/miniconda3/envs/prnn/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjyot/miniconda3/envs/prnn/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 1grams ...\n",
      "You can't omit/backoff and unpack hashtags!\n",
      " unpack_hashtags will be set to False\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ekphrasis parsers\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", # Options are 'twitter' or 'english'\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "spacy_preprocessor = TextPreProcessor(\n",
    "    omit=['email', 'phone', 'user', 'time', 'url', 'date', 'hashtag'],\n",
    "    corrector='twitter',\n",
    "    segmenter='twitter',\n",
    "    tokenizer=lambda x: x.split(),\n",
    "    annotate=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy parser\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_tweet(twt, preprocessor=text_processor, spacy_preprocessor=nlp):\n",
    "    \n",
    "    # Demojize\n",
    "    def demojize(parsed):\n",
    "        if emoji.emoji_count(' '.join(parsed)) > 0:\n",
    "            emoji_parse = []\n",
    "            for tok in parsed:\n",
    "                if emoji.emoji_count(tok) > 0:\n",
    "                    emoji_list = emoji.demojize(tok).strip(':').split('_')\n",
    "                    emoji_list.remove('face')\n",
    "                    emoji_parse.extend(emoji_list)\n",
    "                else:\n",
    "                    emoji_parse.append(tok)\n",
    "        else:\n",
    "            emoji_parse = parsed\n",
    "        return emoji_parse\n",
    "    \n",
    "    def clean_accents(parsed):\n",
    "        return [unidecode.unidecode(x) for x in parsed]\n",
    "    \n",
    "    def clean_tweet_for_spacy(twt):\n",
    "        # Remove emoji, hashtags, user handles and diacritics/accents\n",
    "        # and return the sentence as a string\n",
    "        cleaned = re.sub(r'#\\w+', ' ', twt)\n",
    "        emoji_cleaned = ''\n",
    "        emoji_count = 0\n",
    "        for char in cleaned:\n",
    "            if char in emoji.UNICODE_EMOJI:\n",
    "                emoji_count += 1\n",
    "                continue\n",
    "            emoji_cleaned += char\n",
    "\n",
    "        emoji_cleaned = re.sub(r'\\s\\s+', '', emoji_cleaned)\n",
    "        return emoji_cleaned, emoji_count    \n",
    "    \n",
    "    preprocessed_tweet = clean_accents(demojize(preprocessor.pre_process_doc(twt)))\n",
    "    spacy_raw_text, emoji_count = clean_tweet_for_spacy(twt)\n",
    "    spacy_preprocessed_doc = nlp(spacy_raw_text)\n",
    "    return preprocessed_tweet, spacy_preprocessed_doc, emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moment you bring her to meet your best friend and you're nervous af! ðŸ˜¬ðŸ˜† #nervous #thefriendtest\n",
      "(['The', 'moment', 'you', 'bring', 'her', 'to', 'meet', 'your', 'best', 'friend', 'and', 'you', 'are', 'nervous', 'af', '!', 'grimacing', 'grinning', 'squinting', '<hashtag>', 'nervous', '</hashtag>', '<hashtag>', 'the', 'friend', 'test', '</hashtag>'], The moment you bring her to meet your best friend and you're nervous af!, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample_tweet)\n",
    "print(preprocess_raw_tweet(sample_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekphrasis_parse, spacy_obj, emoji_count = preprocess_raw_tweet(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The moment you bring her to meet your best friend and you're nervous af!"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of tokens\n",
    "def get_no_of_tokens(spacy_obj):\n",
    "    return len(spacy_obj)\n",
    "\n",
    "# Average token length\n",
    "def avg_token_length(spacy_obj):\n",
    "    len_list = [len(x) for x in spacy_obj]\n",
    "    return float(np.mean(np.array(len_list)))\n",
    "    \n",
    "# Upper case token ratio\n",
    "def upper_case_tokens(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.is_upper])/len(spacy_obj)\n",
    "\n",
    "# Title case token ratio\n",
    "def title_case_tokens(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.is_title])/len(spacy_obj)\n",
    "\n",
    "# Get exclamation mark counts\n",
    "def exclamation_mark_count(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.text == '!'])\n",
    "\n",
    "# Get question mark counts\n",
    "def question_mark_count(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.text == '?'])\n",
    "\n",
    "# Get quote mark counts\n",
    "def quote_mark_count(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.text == '\"' or x.text == \"'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0325666666666669"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting 'n' most common tokens for each emotion\n",
    "def get_n_most_valuable_tokens(train):\n",
    "    \n",
    "    n_valuable = (\n",
    "        train\n",
    "        [['tweet', 'emotion', 'intensity']]\n",
    "        .assign(intensity = lambda x: 2*x['intensity'] - 1)\n",
    "        .assign(tweet = lambda x: x['tweet'].str.split()) # Split on whitespace\n",
    "        .explode('tweet')\n",
    "        .assign(tweet = lambda x: x['tweet'].str.replace('[^\\w\\s]','').str.strip().str.lower()) # Remove punctuation, strip, lower\n",
    "        .loc[lambda x: ~x['tweet'].isin(stop_words)]  # Remove stop words\n",
    "        .groupby(by=['tweet', 'emotion'])\n",
    "        .agg({'intensity': 'mean', 'emotion': 'count'})\n",
    "        .rename(columns={'emotion': 'word_count', 'intensity': 'avg_intensity'})\n",
    "        .reset_index()\n",
    "        .assign(abs_intensity = lambda x: np.abs(x['avg_intensity']))\n",
    "        .sort_values(by=['abs_intensity'], ascending=False)\n",
    "        .loc[lambda x: x['word_count'] > 5]\n",
    "        .loc[lambda x: x['abs_intensity'] > 1e-3]\n",
    "        .loc[lambda x: x['tweet'] != '']\n",
    "        \n",
    "    )\n",
    "    return n_valuable\n",
    "\n",
    "def get_n_most_valuable_token_score(spacy_obj, emotion, n_valuable_data):\n",
    "    score = 0\n",
    "    for tok in spacy_obj:\n",
    "        try:\n",
    "            score += n_valuable_data.loc[(n_valuable_data['tweet'] == tok.text) & (n_valuable_data['emotion'] == emotion), 'avg_intensity'].iloc[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return score\n",
    "    \n",
    "n_valuable_data = get_n_most_valuable_tokens(train)\n",
    "\n",
    "get_n_most_valuable_token_score(nlp(sample_tweet), 'fear', n_valuable_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swear words\n",
    "def get_swear_word_list():\n",
    "    with open('../datasets/swear_words.csv', 'r') as f:\n",
    "        swear_words = f.read().split(',')\n",
    "\n",
    "    with open('../datasets/swear_words_2.csv', 'r') as f:\n",
    "        swear_words_2 = f.read().split('\\n')\n",
    "    swear = list(set(swear_words + swear_words_2))\n",
    "    swear.remove('')\n",
    "    return swear\n",
    "\n",
    "swear = get_swear_word_list()\n",
    "\n",
    "def get_no_of_swear_words(spacy_obj, swear=swear):\n",
    "    c = 0\n",
    "    lowercase_tweet = [x.text.lower() for x in spacy_obj]\n",
    "    for t in lowercase_tweet:\n",
    "        if t in swear:\n",
    "            c += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('ad')[0].is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding features\n",
    "\n",
    "def get_embedding_features(spacy_obj):\n",
    "    joy_vec = nlp('joy')[0]\n",
    "    sad_vec = nlp('sad')[0]\n",
    "    anger_vec = nlp('anger')[0]\n",
    "    fear_vec = nlp('fear')[0]\n",
    "    \n",
    "    embed_words = [x for x in spacy_obj if x not in stop_words and x.is_alpha]\n",
    "    \n",
    "    avg_joy_similarity = float(np.mean([x.similarity(joy_vec) for x in embed_words]))\n",
    "    avg_sad_similarity = float(np.mean([x.similarity(sad_vec) for x in embed_words]))\n",
    "    avg_anger_similarity = float(np.mean([x.similarity(anger_vec) for x in embed_words]))\n",
    "    avg_fear_similarity = float(np.mean([x.similarity(fear_vec) for x in embed_words]))\n",
    "    \n",
    "    max_joy_similarity = float(np.max([x.similarity(joy_vec) for x in embed_words]))\n",
    "    max_sad_similarity = float(np.max([x.similarity(sad_vec) for x in embed_words]))\n",
    "    max_anger_similarity = float(np.max([x.similarity(anger_vec) for x in embed_words]))\n",
    "    max_fear_similarity = float(np.max([x.similarity(fear_vec) for x in embed_words]))\n",
    "    \n",
    "    return [avg_joy_similarity, avg_sad_similarity, avg_anger_similarity, avg_fear_similarity,\n",
    "            max_joy_similarity, max_sad_similarity, max_anger_similarity, max_fear_similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3634510636329651,\n",
       " 0.3534475862979889,\n",
       " 0.3017246723175049,\n",
       " 0.38022226095199585,\n",
       " 0.570914626121521,\n",
       " 0.5941110253334045,\n",
       " 0.434806227684021,\n",
       " 0.5261985659599304]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding_features(spacy_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
