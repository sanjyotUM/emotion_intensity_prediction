{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import emoji\n",
    "import unidecode\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 500)\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../datasets/emotion_detection_semeval2017/combined/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>10894</td>\n",
       "      <td>i live and die for mchanzo honeymoon crashing and burning the second they move in together</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2318</td>\n",
       "      <td>30675</td>\n",
       "      <td>All the fans wanted Man Utd at home in the next round...\\nAre you cheering for Northampton or Man Utd right now?\\n#lufc</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>30165</td>\n",
       "      <td>A joyous first webiversary/web mitzvah to Smithsonian's @WeiPoints!! @brianwolly @jackie_mansky @bethpylieberman @bilbo @mazeltov</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3494</td>\n",
       "      <td>21028</td>\n",
       "      <td>@PanicAtTheDisco hey, y'all announced it like immediately after I asked. Nice. Thanks y'all</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1533</td>\n",
       "      <td>40676</td>\n",
       "      <td>Do not be discouraged by a slowing sales market. This will test your business model and pinpoint #strengths and #weaknesses.' @Ken_Dunn</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>10919</td>\n",
       "      <td>Why to have vanity sizes?Now sizes S,XS(evenXXS sometimes) are too big, WTF?! Dear corporate jerks, Lithuania didn't need this. #rant #angry</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2833</td>\n",
       "      <td>20367</td>\n",
       "      <td>@BaileyDemented @hsmitty3 ill kill u if u bully her ðŸ˜¤ðŸ˜¤ðŸ˜¤</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3512</td>\n",
       "      <td>21046</td>\n",
       "      <td>Not sure that men can handle a woman that's got her crap together.  #independent</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2186</td>\n",
       "      <td>30543</td>\n",
       "      <td>lol! no mention of pak PM or even his speech on any international news channel and pakis are rejoicing as if the world stands with them</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>40421</td>\n",
       "      <td>@DxfyingGrxvity - that were rather forlorn, scanning the witches house before resting back on Elphie. 'The Grimmerie is gone.'</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "3650  10894   \n",
       "2318  30675   \n",
       "1808  30165   \n",
       "3494  21028   \n",
       "1533  40676   \n",
       "3675  10919   \n",
       "2833  20367   \n",
       "3512  21046   \n",
       "2186  30543   \n",
       "1278  40421   \n",
       "\n",
       "                                                                                                                                             tweet  \\\n",
       "3650                                                    i live and die for mchanzo honeymoon crashing and burning the second they move in together   \n",
       "2318                       All the fans wanted Man Utd at home in the next round...\\nAre you cheering for Northampton or Man Utd right now?\\n#lufc   \n",
       "1808             A joyous first webiversary/web mitzvah to Smithsonian's @WeiPoints!! @brianwolly @jackie_mansky @bethpylieberman @bilbo @mazeltov   \n",
       "3494                                                  @PanicAtTheDisco hey, y'all announced it like immediately after I asked. Nice. Thanks y'all    \n",
       "1533       Do not be discouraged by a slowing sales market. This will test your business model and pinpoint #strengths and #weaknesses.' @Ken_Dunn   \n",
       "3675  Why to have vanity sizes?Now sizes S,XS(evenXXS sometimes) are too big, WTF?! Dear corporate jerks, Lithuania didn't need this. #rant #angry   \n",
       "2833                                                                                       @BaileyDemented @hsmitty3 ill kill u if u bully her ðŸ˜¤ðŸ˜¤ðŸ˜¤   \n",
       "3512                                                              Not sure that men can handle a woman that's got her crap together.  #independent   \n",
       "2186       lol! no mention of pak PM or even his speech on any international news channel and pakis are rejoicing as if the world stands with them   \n",
       "1278                @DxfyingGrxvity - that were rather forlorn, scanning the witches house before resting back on Elphie. 'The Grimmerie is gone.'   \n",
       "\n",
       "      emotion  intensity  \n",
       "3650    anger      0.479  \n",
       "2318      joy      0.292  \n",
       "1808      joy      0.667  \n",
       "3494     fear      0.250  \n",
       "1533  sadness      0.271  \n",
       "3675    anger      0.708  \n",
       "2833     fear      0.583  \n",
       "3512     fear      0.229  \n",
       "2186      joy      0.396  \n",
       "1278  sadness      0.458  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweet = \"The moment you bring her to meet your best friend and you're nervous af! ðŸ˜¬ðŸ˜† #nervous #thefriendtest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjyot/miniconda3/envs/prnn/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjyot/miniconda3/envs/prnn/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 1grams ...\n",
      "You can't omit/backoff and unpack hashtags!\n",
      " unpack_hashtags will be set to False\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ekphrasis parsers\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", # Options are 'twitter' or 'english'\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=False).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "spacy_preprocessor = TextPreProcessor(\n",
    "    omit=['email', 'phone', 'user', 'time', 'url', 'date', 'hashtag'],\n",
    "    corrector='twitter',\n",
    "    segmenter='twitter',\n",
    "    tokenizer=lambda x: x.split(),\n",
    "    annotate=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy parser\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'moment',\n",
       " 'you',\n",
       " 'bring',\n",
       " 'her',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'your',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'you',\n",
       " 'are',\n",
       " 'nervous',\n",
       " 'af',\n",
       " '!',\n",
       " 'grimacing',\n",
       " 'grinning',\n",
       " 'squinting',\n",
       " '<hashtag>',\n",
       " 'nervous',\n",
       " '</hashtag>',\n",
       " '<hashtag>',\n",
       " 'the',\n",
       " 'friend',\n",
       " 'test',\n",
       " '</hashtag>']"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, _, _ = preprocess_raw_tweet(sample_tweet)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_tweet(twt, preprocessor=text_processor, spacy_preprocessor=nlp):\n",
    "    \n",
    "    # Demojize\n",
    "    def demojize(parsed):\n",
    "        if emoji.emoji_count(' '.join(parsed)) > 0:\n",
    "            emoji_parse = []\n",
    "            for tok in parsed:\n",
    "                if emoji.emoji_count(tok) > 0:\n",
    "                    emoji_list = emoji.demojize(tok).strip(':').split('_')\n",
    "                    emoji_list.remove('face')\n",
    "                    emoji_parse.extend(emoji_list)\n",
    "                else:\n",
    "                    emoji_parse.append(tok)\n",
    "        else:\n",
    "            emoji_parse = parsed\n",
    "        return emoji_parse\n",
    "    \n",
    "    def clean_accents(parsed):\n",
    "        return [unidecode.unidecode(x) for x in parsed]\n",
    "    \n",
    "    def clean_tweet_for_spacy(twt):\n",
    "        # Remove emoji, hashtags, user handles and diacritics/accents\n",
    "        # and return the sentence as a string\n",
    "        cleaned = re.sub(r'#\\w+', ' ', twt)\n",
    "        emoji_cleaned = ''\n",
    "        emoji_count = 0\n",
    "        for char in cleaned:\n",
    "            if char in emoji.UNICODE_EMOJI:\n",
    "                emoji_count += 1\n",
    "                continue\n",
    "            emoji_cleaned += char\n",
    "\n",
    "        emoji_cleaned = re.sub(r'\\s\\s+', ' ', emoji_cleaned)\n",
    "        return emoji_cleaned, emoji_count\n",
    "    \n",
    "    def extract_hashtag(ekphrasis_parse):\n",
    "        hashtag_tokens = []\n",
    "        append_flag = False\n",
    "        for i, tok in enumerate(ekphrasis_parse):\n",
    "            if tok == '<hashtag>':\n",
    "                append_flag = True\n",
    "            if tok == '</hashtag>':\n",
    "                append_flag = False\n",
    "\n",
    "            if append_flag and ekphrasis_parse[i+1] != '</hashtag>':\n",
    "                hashtag_tokens.append(ekphrasis_parse[i+1])\n",
    "        return hashtag_tokens\n",
    "    \n",
    "    preprocessed_tweet = clean_accents(demojize(preprocessor.pre_process_doc(twt)))\n",
    "    hashtag_list = extract_hashtag(preprocessed_tweet)\n",
    "    spacy_raw_text, emoji_count = clean_tweet_for_spacy(twt)\n",
    "    text_and_hashtag = spacy_raw_text + ' ' + ' '.join(hashtag_list)\n",
    "    spacy_preprocessed_doc = nlp(text_and_hashtag)\n",
    "    return preprocessed_tweet, spacy_preprocessed_doc, emoji_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moment you bring her to meet your best friend and you're nervous af! ðŸ˜¬ðŸ˜† #nervous #thefriendtest\n",
      "(['The', 'moment', 'you', 'bring', 'her', 'to', 'meet', 'your', 'best', 'friend', 'and', 'you', 'are', 'nervous', 'af', '!', 'grimacing', 'grinning', 'squinting', '<hashtag>', 'nervous', '</hashtag>', '<hashtag>', 'the', 'friend', 'test', '</hashtag>'], The moment you bring her to meet your best friend and you're nervous af!  nervous the friend test, 2)\n"
     ]
    }
   ],
   "source": [
    "print(sample_tweet)\n",
    "print(preprocess_raw_tweet(sample_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "ekphrasis_parse, spacy_obj, emoji_count = preprocess_raw_tweet(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The moment you bring her to meet your best friend and you're nervous af! nervous thefriendtest"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of tokens\n",
    "def get_no_of_tokens(spacy_obj):\n",
    "    return len(spacy_obj)\n",
    "\n",
    "# Average token length\n",
    "def avg_token_length(spacy_obj):\n",
    "    len_list = [len(x) for x in spacy_obj]\n",
    "    return float(np.mean(np.array(len_list)))\n",
    "    \n",
    "# Upper case token ratio\n",
    "def upper_case_tokens(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.is_upper])/len(spacy_obj)\n",
    "\n",
    "# Title case token ratio\n",
    "def title_case_tokens(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.is_title])/len(spacy_obj)\n",
    "\n",
    "# Get exclamation mark counts\n",
    "def exclamation_mark_count(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.text == '!'])\n",
    "\n",
    "# Get question mark counts\n",
    "def question_mark_count(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.text == '?'])\n",
    "\n",
    "# Get quote mark counts\n",
    "def quote_mark_count(spacy_obj):\n",
    "    return len([x for x in spacy_obj if x.text == '\"' or x.text == \"'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0325666666666669"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting 'n' most common tokens for each emotion\n",
    "def get_n_most_valuable_tokens(train):\n",
    "    \n",
    "    n_valuable = (\n",
    "        train\n",
    "        [['tweet', 'emotion', 'intensity']]\n",
    "        .assign(intensity = lambda x: 2*x['intensity'] - 1)\n",
    "        .assign(tweet = lambda x: x['tweet'].str.split()) # Split on whitespace\n",
    "        .explode('tweet')\n",
    "        .assign(tweet = lambda x: x['tweet'].str.replace('[^\\w\\s]','').str.strip().str.lower()) # Remove punctuation, strip, lower\n",
    "        .loc[lambda x: ~x['tweet'].isin(stop_words)]  # Remove stop words\n",
    "        .groupby(by=['tweet', 'emotion'])\n",
    "        .agg({'intensity': 'mean', 'emotion': 'count'})\n",
    "        .rename(columns={'emotion': 'word_count', 'intensity': 'avg_intensity'})\n",
    "        .reset_index()\n",
    "        .assign(abs_intensity = lambda x: np.abs(x['avg_intensity']))\n",
    "        .sort_values(by=['abs_intensity'], ascending=False)\n",
    "        .loc[lambda x: x['word_count'] > 5]\n",
    "        .loc[lambda x: x['abs_intensity'] > 1e-3]\n",
    "        .loc[lambda x: x['tweet'] != '']\n",
    "        \n",
    "    )\n",
    "    return n_valuable\n",
    "\n",
    "def get_n_most_valuable_token_score(spacy_obj, emotion, n_valuable_data):\n",
    "    score = 0\n",
    "    for tok in spacy_obj:\n",
    "        try:\n",
    "            score += n_valuable_data.loc[(n_valuable_data['tweet'] == tok.text) & (n_valuable_data['emotion'] == emotion), 'avg_intensity'].iloc[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return score\n",
    "    \n",
    "n_valuable_data = get_n_most_valuable_tokens(train)\n",
    "\n",
    "get_n_most_valuable_token_score(nlp(sample_tweet), 'fear', n_valuable_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presence of synonyms and antonyms of the emotion labels\n",
    "def syn_ant_score(spacy_obj):\n",
    "    ps= PorterStemmer()\n",
    "    \n",
    "    joy_syn = [\"joy\",\"happy\",\"content\",\"statisfy\",\"satisfactory\",\"amusement\",\"bliss\",\"charm\",\"cheer\",\"comfort\",\"delight\",\"elation\",\"glee\",\"humor\",\"pride\",\"satisfaction\",\"wonder\",\"alleviatin\",\"animation\",\"delectation\",\"diversion\",\"ecstasy\",\"exultation\",\"exulting\",\"felicity\",\"festivity\",\"frolic\",\"fruition\",\"gaiety\",\"gem\",\"gladness\",\"gratification\",\"hilarity\",\"indulgence\",\"jewel\",\"jubilance\",\"liveliness\",\"luxury\",\"merriment\",\"mirth\",\"prize\",\"rapture\",\"ravishment\",\"refreshment\",\"rejoicing\",\"revelry\",\"solace\",\"sport\",\"transport\",\"treasure\",\"treat\",\"good humor\",\"pride and joy\",\"regalement\"]\n",
    "    joy_ant = [\"sad\",\"depression\",\"melancholy\",\"misery\",\"sadness\",\"seriousness\",\"sorrow\",\"unhappiness\",\"discouragement\",\"dislike\",\"mourning\",\"vocation\",\"work\",\"woe\"]\n",
    "    sad_syn= [\"sad\",\"bitter\",\"dismal\",\"heartbroken\",\"melancholy\",\"mournful\",\"pessimistic\",\"somber\",\"sorrowful\",\"sorry\",\"wistful\",\"bereaved\",\"blue\",\"cheerless\",\"dejected\",\"despairing\",\"despondent\",\"disconsolate\",\"distressed\",\"doleful\",\"down\",\"down in dumps\",\"down in mouth\",\"downcast\",\"forlorn\",\"gloomy\",\"glum\",\"grief-stricken\",\"grieved\",\"heartsick\",\"heavyhearted\",\"hurting\",\"in doldrums\",\"in grief\",\"in the dumps\",\"languishing\",\"low\",\"low-spirited\",\"lugubrious\",\"morbid\",\"morose\",\"out of sorts\",\"pensive\",\"sick at heart\",\"troubled\",\"weeping\",\"woebegone\"]\n",
    "    sad_ant = [\"joy\",\"satisfy\",\"delightful\",\"glad\",\"good\",\"happy\",\"hopeful\",\"joyful\",\"nice\",\"pleasant\",\"fortunate\",\"great\",\"lucky\"]\n",
    "    anger_syn = [\"acrimony\",\"animosity\",\"annoyance\",\"antagonism\",\"displeasure\",\"enmity\",\"exasperation\",\"fury\",\"hatred\",\"impatience\",\"indignation\",\"ire\",\"irritation\",\"outrage\",\"passion\",\"rage\",\"resentment\",\"temper\",\"violence\",\"chagrin\",\"choler\",\"conniption\",\"dander\",\"disapprobation\",\"distemper\",\"gall\",\"huff\",\"infuriation\",\"irascibility\",\"irritability\",\"miff\",\"peevishness\",\"petulance\",\"pique\",\"rankling\",\"soreness\",\"stew\",\"storm\",\"tantrum\",\"tiff\",\"umbrage\",\"vexation\",\"blow up\",\"cat fit\",\"hissy fit\",\"ill humor\",\"ill temper\",\"mad\",\"slow burn\"]\n",
    "    anger_ant = [\"calm\",\"calmness\",\"cheer\",\"comfort\",\"delight\",\"ease\",\"glee\",\"good will\",\"happiness\",\"joy\",\"kindness\",\"liking\",\"love\",\"peace\",\"pleasure\",\"agreeability\",\"contentment\",\"enjoyment\",\"good nature\",\"pleasantness\"]\n",
    "    fear_syn= [\"angst\",\"anxiety\",\"concern\",\"despair\",\"dismay\",\"doubt\",\"dread\",\"horror\",\"jitters\",\"panic\",\"scare\",\"suspicion\",\"terror\",\"unease\",\"uneasiness\",\"worry\",\"abhorrence\",\"agitation\",\"aversion\",\"awe\",\"consternation\",\"cowardice\",\"creeps\",\"discomposure\",\"disquietude\",\"distress\",\"faintheartedness\",\"foreboding\",\"fright\",\"funk\",\"misgiving\",\"nightmare\",\"phobia\",\"presentiment\",\"qualm\",\"reverence\",\"revulsion\",\"timidity\",\"trembling\",\"tremor\",\"trepidation\",\"bÃªte noire\",\"chickenheartedness\",\"cold feet\",\"cold sweat\",\"recreancy\"]\n",
    "    fear_ant = [\"assurance\",\"calmness\",\"cheer\",\"confidence\",\"contentment\",\"ease\",\"encouragement\",\"faith\",\"happiness\",\"joy\",\"trust\",\"calm\",\"comfort\",\"like\",\"liking\",\"love\",\"bravery\",\"courage\",\"fearlessness\",\"heroism\",\"unconcern\"]\n",
    "\n",
    "    joy_syn = [ps.stem(x) for x in joy_syn]\n",
    "    joy_ant = [ps.stem(x) for x in joy_ant]\n",
    "    sad_syn = [ps.stem(x) for x in sad_syn]\n",
    "    sad_ant = [ps.stem(x) for x in sad_ant]\n",
    "    anger_syn = [ps.stem(x) for x in anger_syn]\n",
    "    anger_ant = [ps.stem(x) for x in anger_ant]\n",
    "    fear_syn = [ps.stem(x) for x in fear_syn]\n",
    "    fear_ant = [ps.stem(x) for x in fear_ant]\n",
    "\n",
    "    joy_score = 0\n",
    "    sad_score = 0\n",
    "    fear_score = 0\n",
    "    anger_score = 0\n",
    "    \n",
    "    for tok in spacy_obj:\n",
    "        st_tok = ps.stem(tok.text.lower())\n",
    "        \n",
    "        if st_tok in joy_syn:\n",
    "            joy_score += 1\n",
    "        if st_tok in joy_ant:\n",
    "            joy_score -= 1\n",
    "        if st_tok in sad_syn:\n",
    "            sad_score += 1\n",
    "        if st_tok in sad_ant:\n",
    "            sad_score -= 1\n",
    "        if st_tok in fear_syn:\n",
    "            fear_score += 1\n",
    "        if st_tok in fear_ant:\n",
    "            fear_score -= 1\n",
    "        if st_tok in anger_syn:\n",
    "            anger_score += 1\n",
    "        if st_tok in anger_ant:\n",
    "            anger_score -= 1\n",
    "    \n",
    "    return joy_score, sad_score, fear_score, anger_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swear words\n",
    "def get_swear_word_list():\n",
    "    with open('../datasets/swear_words.csv', 'r') as f:\n",
    "        swear_words = f.read().split(',')\n",
    "\n",
    "    with open('../datasets/swear_words_2.csv', 'r') as f:\n",
    "        swear_words_2 = f.read().split('\\n')\n",
    "    swear = list(set(swear_words + swear_words_2))\n",
    "    swear.remove('')\n",
    "    return swear\n",
    "\n",
    "swear = get_swear_word_list()\n",
    "\n",
    "def get_no_of_swear_words(spacy_obj, swear=swear):\n",
    "    c = 0\n",
    "    lowercase_tweet = [x.text.lower() for x in spacy_obj]\n",
    "    for t in lowercase_tweet:\n",
    "        if t in swear:\n",
    "            c += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding features\n",
    "\n",
    "def get_embedding_features(spacy_obj):\n",
    "    joy_vec = nlp('joy')[0]\n",
    "    sad_vec = nlp('sad')[0]\n",
    "    anger_vec = nlp('anger')[0]\n",
    "    fear_vec = nlp('fear')[0]\n",
    "    \n",
    "    embed_words = [x for x in spacy_obj if x not in stop_words and x.is_alpha]\n",
    "    \n",
    "    avg_joy_similarity = float(np.mean([x.similarity(joy_vec) for x in embed_words]))\n",
    "    avg_sad_similarity = float(np.mean([x.similarity(sad_vec) for x in embed_words]))\n",
    "    avg_anger_similarity = float(np.mean([x.similarity(anger_vec) for x in embed_words]))\n",
    "    avg_fear_similarity = float(np.mean([x.similarity(fear_vec) for x in embed_words]))\n",
    "    \n",
    "    max_joy_similarity = float(np.max([x.similarity(joy_vec) for x in embed_words]))\n",
    "    max_sad_similarity = float(np.max([x.similarity(sad_vec) for x in embed_words]))\n",
    "    max_anger_similarity = float(np.max([x.similarity(anger_vec) for x in embed_words]))\n",
    "    max_fear_similarity = float(np.max([x.similarity(fear_vec) for x in embed_words]))\n",
    "    \n",
    "    return [avg_joy_similarity, avg_sad_similarity, avg_anger_similarity, avg_fear_similarity,\n",
    "            max_joy_similarity, max_sad_similarity, max_anger_similarity, max_fear_similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3634510636329651,\n",
       " 0.3534475862979889,\n",
       " 0.3017246723175049,\n",
       " 0.38022226095199585,\n",
       " 0.570914626121521,\n",
       " 0.5941110253334045,\n",
       " 0.434806227684021,\n",
       " 0.5261985659599304]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embedding_features(spacy_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The moment you bring her to meet your best friend and you're nervous af!"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affect intensity score\n",
    "affect_score_filepath = '../datasets/lexicon_sentiments/NRC-Sentiment-Emotion-Lexicons/NRC-Affect-Intensity-Lexicon/NRC-AffectIntensity-Lexicon.txt'\n",
    "score_df = pd.read_csv(affect_score_filepath, sep='\\t')\n",
    "\n",
    "def get_affect_intensity_score(spacy_obj, emotion, score_df=score_df):\n",
    "    score = 0\n",
    "    for tok in spacy_obj:\n",
    "        try:\n",
    "            score += score_df.loc[(score_df['term'] == tok.text.lower()) & (score_df['AffectDimension'] == emotion), 'score'].iloc[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lexicon senselevel data\n",
    "\n",
    "senselevel_data_path = '../datasets/lexicon_sentiments/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Senselevel-v0.92.txt'\n",
    "senselevel_data_score_df = (\n",
    "    pd.read_csv(senselevel_data_path, sep='\\t', names=['word', 'emotion', 'score'])\n",
    "    .assign(word = lambda x: x['word'].str.split('--').str[0])\n",
    "    .drop_duplicates()\n",
    "    .loc[lambda x: x['score'] > 0]\n",
    "    .loc[lambda x: x['emotion'].isin(['fear', 'anger', 'joy', 'sadness'])]      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lexicon senselevel data score\n",
    "def get_lexicon_senselevel_data_score(spacy_obj, emotion, score_df=senselevel_data_score_df):\n",
    "    score = 0\n",
    "    for tok in spacy_obj:\n",
    "        try:\n",
    "            score += score_df.loc[(score_df['word'] == tok.text.lower()) & (score_df['emotion'] == emotion), 'score'].iloc[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return score\n",
    "    \n",
    "get_lexicon_senselevel_data_score(spacy_obj, 'fear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect features into single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_features(df, emotion):\n",
    "    \n",
    "    edf = df[df['emotion'] == emotion]\n",
    "    \n",
    "    def initialize_feat_dict():\n",
    "        id_cols = ['id', 'intensity']\n",
    "        syn_features = ['no_of_tokens', 'avg_token_len', 'upper_case_tokens', 'title_case_tokens', \n",
    "                    'exclamation_mark_count', 'question_mark_count', 'quote_mark_count', \n",
    "                    'emoji_count']\n",
    "        sem_features = ['n_valuable_tok_score', 'joy_syn_ant_score', 'sad_syn_ant_score', \n",
    "                        'fear_syn_ant_score', 'anger_syn_ant_score', 'cuss_count']\n",
    "        embedding_features = ['avg_joy_sim', 'avg_sad_sim', 'avg_anger_sim', 'avg_fear_sim', \n",
    "                              'max_joy_sim', 'max_sad_sim', 'max_anger_sim', 'max_fear_sim']\n",
    "        affect_data_features = ['joy_affect_score', 'sad_affect_score', 'anger_affect_score', 'fear_affect_score']\n",
    "        \n",
    "        senselevel_data_features = ['joy_senselevel_score', 'sad_senselevel_score', 'anger_senselevel_score', 'fear_senselevel_score']\n",
    "        \n",
    "        feat = dict()\n",
    "        for col in id_cols + syn_features + sem_features + embedding_features + affect_data_features + senselevel_data_features:\n",
    "            feat[col] = list()\n",
    "            \n",
    "        return feat\n",
    "    \n",
    "    feat = initialize_feat_dict()  # Dictionary to store results\n",
    "    \n",
    "    for _, row in edf.iterrows():\n",
    "        row = row.to_dict()\n",
    "        tweet_id = row['id']\n",
    "        tweet = row['tweet']\n",
    "        intensity = row['intensity']\n",
    "        \n",
    "        feat['id'] = tweet_id\n",
    "        feat['intensity'] = intensity\n",
    "        \n",
    "        annotated_tweet, spacy_obj, emoji_count = preprocess_raw_tweet(tweet)\n",
    "        \n",
    "        # Syntactic features\n",
    "        feat['no_of_tokens'].append(get_no_of_tokens(spacy_obj))\n",
    "        feat['avg_token_len'].append(avg_token_length(spacy_obj))\n",
    "        feat['upper_case_tokens'].append(upper_case_tokens(spacy_obj))\n",
    "        feat['title_case_tokens'].append(title_case_tokens(spacy_obj))\n",
    "        feat['exclamation_mark_count'].append(exclamation_mark_count(spacy_obj))\n",
    "        feat['question_mark_count'].append(question_mark_count(spacy_obj))\n",
    "        feat['quote_mark_count'].append(quote_mark_count(spacy_obj))\n",
    "        feat['emoji_count'].append(emoji_count)\n",
    "        \n",
    "        # Semantic features\n",
    "        feat['n_valuable_tok_score'].append(get_n_most_valuable_token_score(spacy_obj, emotion, n_valuable_data=n_valuable_data))\n",
    "        joy_syn_ant_score, sad_syn_ant_score, fear_syn_ant_score, anger_syn_ant_score = syn_ant_score(spacy_obj)\n",
    "        feat['joy_syn_ant_score'].append(joy_syn_ant_score)\n",
    "        feat['sad_syn_ant_score'].append(sad_syn_ant_score)\n",
    "        feat['fear_syn_ant_score'].append(fear_syn_ant_score)\n",
    "        feat['anger_syn_ant_score'].append(anger_syn_ant_score)\n",
    "        feat['cuss_count'].append(get_no_of_swear_words(spacy_obj))\n",
    "        \n",
    "        # Embedding features\n",
    "        avg_joy_sim, avg_sad_sim, avg_anger_sim, avg_fear_sim, \\\n",
    "        max_joy_sim, max_sad_sim, max_anger_sim, max_fear_sim = get_embedding_features(spacy_obj)\n",
    "        feat['avg_joy_sim'].append(avg_joy_sim)\n",
    "        feat['avg_sad_sim'].append(avg_sad_sim)\n",
    "        feat['avg_anger_sim'].append(avg_anger_sim)\n",
    "        feat['avg_fear_sim'].append(avg_fear_sim)\n",
    "        feat['max_joy_sim'].append(max_joy_sim)\n",
    "        feat['max_sad_sim'].append(max_sad_sim)\n",
    "        feat['max_anger_sim'].append(max_anger_sim)\n",
    "        feat['max_fear_sim'].append(max_fear_sim)\n",
    "        \n",
    "        # Affect data intensity score\n",
    "        feat['joy_affect_score'].append(get_affect_intensity_score(spacy_obj, 'joy'))\n",
    "        feat['sad_affect_score'].append(get_affect_intensity_score(spacy_obj, 'sadness'))\n",
    "        feat['anger_affect_score'].append(get_affect_intensity_score(spacy_obj, 'anger'))\n",
    "        feat['fear_affect_score'].append(get_affect_intensity_score(spacy_obj, 'fear'))\n",
    "        \n",
    "        # Get lexicon senselevel data score\n",
    "        feat['joy_senselevel_score'].append(get_lexicon_senselevel_data_score(spacy_obj, 'joy'))\n",
    "        feat['sad_senselevel_score'].append(get_lexicon_senselevel_data_score(spacy_obj, 'sadness'))\n",
    "        feat['anger_senselevel_score'].append(get_lexicon_senselevel_data_score(spacy_obj, 'anger'))\n",
    "        feat['fear_senselevel_score'].append(get_lexicon_senselevel_data_score(spacy_obj, 'fear'))\n",
    "    return pd.DataFrame(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!... should I knock the landlord door. #angry #mad ##</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone the N word. If I wasn't in a moving vehicle I'd have jumped out #disgusted</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered to a pick up store not my address #fuming #poorcustomerservice</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alarm in davis bc I was sound asleep #pissed #angry #upset #tired #sad #tired #hangry ######</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on you, talk over you and are rude. Taking money out of my acc willynilly! #fuming</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  10000   \n",
       "1  10001   \n",
       "2  10002   \n",
       "3  10003   \n",
       "4  10004   \n",
       "\n",
       "                                                                                                                                    tweet  \\\n",
       "0                                        How the fu*k! Who the heck! moved my fridge!... should I knock the landlord door. #angry #mad ##   \n",
       "1                So my Indian Uber driver just called someone the N word. If I wasn't in a moving vehicle I'd have jumped out #disgusted    \n",
       "2                            @DPD_UK I asked for my parcel to be delivered to a pick up store not my address #fuming #poorcustomerservice   \n",
       "3  so ef whichever butt wipe pulled the fire alarm in davis bc I was sound asleep #pissed #angry #upset #tired #sad #tired #hangry ######   \n",
       "4           Don't join @BTCare they put the phone down on you, talk over you and are rude. Taking money out of my acc willynilly! #fuming   \n",
       "\n",
       "  emotion  intensity  \n",
       "0   anger      0.938  \n",
       "1   anger      0.896  \n",
       "2   anger      0.896  \n",
       "3   anger      0.896  \n",
       "4   anger      0.896  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>intensity</th>\n",
       "      <th>no_of_tokens</th>\n",
       "      <th>avg_token_len</th>\n",
       "      <th>upper_case_tokens</th>\n",
       "      <th>title_case_tokens</th>\n",
       "      <th>exclamation_mark_count</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>quote_mark_count</th>\n",
       "      <th>emoji_count</th>\n",
       "      <th>...</th>\n",
       "      <th>max_anger_sim</th>\n",
       "      <th>max_fear_sim</th>\n",
       "      <th>joy_affect_score</th>\n",
       "      <th>sad_affect_score</th>\n",
       "      <th>anger_affect_score</th>\n",
       "      <th>fear_affect_score</th>\n",
       "      <th>joy_senselevel_score</th>\n",
       "      <th>sad_senselevel_score</th>\n",
       "      <th>anger_senselevel_score</th>\n",
       "      <th>fear_senselevel_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10002</td>\n",
       "      <td>0.896</td>\n",
       "      <td>24</td>\n",
       "      <td>3.208333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683652</td>\n",
       "      <td>0.574110</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.512</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10002</td>\n",
       "      <td>0.896</td>\n",
       "      <td>27</td>\n",
       "      <td>3.592593</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0.499185</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10002</td>\n",
       "      <td>0.896</td>\n",
       "      <td>22</td>\n",
       "      <td>4.045455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433860</td>\n",
       "      <td>0.532098</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  intensity  no_of_tokens  avg_token_len  upper_case_tokens  \\\n",
       "0  10002      0.896            24       3.208333           0.041667   \n",
       "1  10002      0.896            27       3.592593           0.111111   \n",
       "2  10002      0.896            22       4.045455           0.090909   \n",
       "\n",
       "   title_case_tokens  exclamation_mark_count  question_mark_count  \\\n",
       "0           0.125000                       3                    0   \n",
       "1           0.259259                       0                    0   \n",
       "2           0.045455                       0                    0   \n",
       "\n",
       "   quote_mark_count  emoji_count  ...  max_anger_sim  max_fear_sim  \\\n",
       "0                 0            0  ...       0.683652      0.574110   \n",
       "1                 0            0  ...       0.465649      0.499185   \n",
       "2                 0            0  ...       0.433860      0.532098   \n",
       "\n",
       "   joy_affect_score  sad_affect_score  anger_affect_score  fear_affect_score  \\\n",
       "0             0.000             0.500               1.512              0.547   \n",
       "1             0.000             0.295               0.000              0.000   \n",
       "2             0.172             0.000               0.812              0.000   \n",
       "\n",
       "   joy_senselevel_score  sad_senselevel_score  anger_senselevel_score  \\\n",
       "0                     0                     1                       2   \n",
       "1                     0                     0                       0   \n",
       "2                     0                     0                       1   \n",
       "\n",
       "   fear_senselevel_score  \n",
       "0                      1  \n",
       "1                      0  \n",
       "2                      0  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_all_features(train.head(3), 'anger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
